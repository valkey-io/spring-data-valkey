name: Benchmark

on:
  workflow_dispatch:
    inputs:
      clients:
        description: 'Client configurations (comma-separated, e.g., "valkey_glide,lettuce" or "all")'
        required: true
        type: string
        default: 'valkey_glide'
      workloads:
        description: 'Workload configurations (comma-separated, e.g., "standard_workload,high_throughput" or "all")'
        required: true
        type: string
        default: 'standard_workload'

  push:
    branches:
      - add-benchmark-ci

env:
  # Define all available configurations
  ALL_CLIENTS: 'valkey_glide'
  ALL_WORKLOADS: 'standard_workload'
  # Defaults for push events
  DEFAULT_CLIENTS: 'valkey_glide'
  DEFAULT_WORKLOADS: 'standard_workload'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4

      - name: Generate benchmark matrix
        id: set-matrix
        run: |
          # Get input or use default
          INPUT_CLIENTS="${{ github.event.inputs.clients }}"
          INPUT_WORKLOADS="${{ github.event.inputs.workloads }}"
          
          SELECTED_CLIENTS="${INPUT_CLIENTS:-${{ env.DEFAULT_CLIENTS }}}"
          SELECTED_WORKLOADS="${INPUT_WORKLOADS:-${{ env.DEFAULT_WORKLOADS }}}"
          
          echo "Selected clients: $SELECTED_CLIENTS"
          echo "Selected workloads: $SELECTED_WORKLOADS"
          
          # Handle "all" keyword
          if [ "$SELECTED_CLIENTS" == "all" ]; then
            SELECTED_CLIENTS="${{ env.ALL_CLIENTS }}"
          fi
          
          if [ "$SELECTED_WORKLOADS" == "all" ]; then
            SELECTED_WORKLOADS="${{ env.ALL_WORKLOADS }}"
          fi
          
          # Convert comma-separated to JSON array
          # e.g., "valkey_glide,lettuce" -> ["valkey_glide","lettuce"]
          CLIENTS_JSON=$(echo "$SELECTED_CLIENTS" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | jq -R . | jq -sc .)
          WORKLOADS_JSON=$(echo "$SELECTED_WORKLOADS" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | jq -R . | jq -sc .)
          
          # Build matrix JSON
          MATRIX=$(jq -c -n \
            --argjson clients "$CLIENTS_JSON" \
            --argjson workloads "$WORKLOADS_JSON" \
            '{client: $clients, workload: $workloads}')
          
          echo "Generated matrix: $MATRIX"
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  benchmark:
    needs: setup
    runs-on: [self-hosted, Linux, x86, ephemeral, metal]
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - name: Display job info
        run: |
          echo "========================================"
          echo "Benchmark Job"
          echo "========================================"
          echo "Client: ${{ matrix.client }}"
          echo "Workload: ${{ matrix.workload }}"
          echo "========================================"
          
          echo "Workload config:"
          cat ".github/workflows/configs/workloads/${{ matrix.workload }}.json" | jq '.phases[] | {id, completion}'

      - name: Verify metal instance
        run: |
          echo "=== System Info ==="
          lscpu | grep "Model name"
          echo "CPU cores: $(nproc)"
          
          VIRT=$(systemd-detect-virt 2>/dev/null || echo "none")
          echo "Virtualization: $VIRT"
          if [ "$VIRT" != "none" ]; then
            echo "WARNING: Not running on bare metal!"
          fi

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y sysstat iproute2 python3-pip
          
          # Install Python dependencies system-wide
          sudo python3 -m pip install psycopg2-binary boto3

          # Install FlameGraph tools
          if [ ! -d "/opt/FlameGraph" ]; then
            git clone --depth 1 https://github.com/brendangregg/FlameGraph.git /tmp/FlameGraph
            sudo mv /tmp/FlameGraph /opt/FlameGraph
          fi
          ls /opt/FlameGraph/stackcollapse-perf.pl && echo "✓ FlameGraph tools installed"
          
          CODENAME=$(lsb_release -cs)
          KERNEL=$(uname -r)
          echo "Ubuntu: $CODENAME, Kernel: $KERNEL"
          
          if sudo apt-get install -y linux-tools-$(uname -r) 2>/dev/null; then
            echo "✓ Installed kernel-matched perf tools"
          else
            echo "Kernel-matched tools not available, trying HWE fallback..."
            
            cd /tmp
            
            KERNEL_MAJOR=$(echo $KERNEL | cut -d'.' -f1,2)
            echo "Looking for HWE tools for kernel $KERNEL_MAJOR..."
            
            HWE_PKG=$(apt-cache search "linux-hwe.*tools.*${KERNEL_MAJOR}" 2>/dev/null | \
                      grep -oP "linux-hwe-[\d.]+-tools-[\d.]+-\d+" | \
                      sort -V | tail -1)
            
            if [ -n "$HWE_PKG" ]; then
              echo "Found HWE package: $HWE_PKG"
              apt-get download "$HWE_PKG"
            else
              echo "No matching HWE package found, trying generic..."
              sudo apt-get install -y linux-tools-generic 2>/dev/null || true
            fi
            
            for deb in linux-hwe*tools*.deb; do
              if [ -f "$deb" ]; then
                echo "Extracting $deb..."
                sudo mkdir -p /opt/perf-tools
                sudo dpkg -x "$deb" /opt/perf-tools
                PERF_BIN=$(find /opt/perf-tools -name "perf" -type f 2>/dev/null | head -1)
                if [ -n "$PERF_BIN" ]; then
                  sudo ln -sf "$PERF_BIN" /usr/local/bin/perf
                  echo "✓ Linked perf from $PERF_BIN"
                fi
                rm -f "$deb"
                break
              fi
            done
            
            cd -
          fi
          
          # Set perf permissions before verification
          sudo sysctl -w kernel.perf_event_paranoid=-1
          sudo sysctl -w kernel.kptr_restrict=0
          
          echo ""
          echo "=== Perf Verification ==="
          which perf
          perf --version
          perf stat -e task-clock -- sleep 0.1
          echo "✓ Perf installed and working"

      - name: Run benchmark
        run: |
          python3 .github/scripts/benchmark_runner.py \
            --output "benchmark_results_${{ matrix.client }}_${{ matrix.workload }}.json" \
            --workload-config ".github/workflows/configs/workloads/${{ matrix.workload }}.json" \
            --client-config ".github/workflows/configs/clients/${{ matrix.client }}.json" \
            --project-dir ${{ github.workspace }}

      - name: Display results summary
        if: always()
        run: |
          RESULT_FILE="benchmark_results_${{ matrix.client }}_${{ matrix.workload }}.json"
          if [ -f "$RESULT_FILE" ]; then
            echo "=== Benchmark Results Summary ==="
            python3 -c "
          import json
          with open('$RESULT_FILE') as f:
              data = json.load(f)
          
          print(f\"Job ID: {data['job_id']}\")
          print(f\"Client: {data['config']['client']['client_name']}\")
          print(f\"Workload: {data['config']['workload']['benchmark-profile']['name']}\")
          print(f\"Elapsed: {data['results']['elapsed_seconds']}s\")
          print()
          print('Operations:')
          for op, stats in data['results']['operations'].items():
              success_rate = (stats['successful_requests'] / stats['total_requests'] * 100) if stats['total_requests'] > 0 else 0
              print(f\"  {op}: {stats['total_requests']:,} requests ({success_rate:.2f}% success)\")
          print()
          print('Perf:')
          perf = data['results']['perf']['counters']
          if perf.get('ipc'):
              print(f\"  IPC: {perf['ipc']}\")
          if perf.get('cache_miss_rate'):
              print(f\"  Cache miss rate: {perf['cache_miss_rate']}%\")
          if perf.get('branch_miss_rate'):
              print(f\"  Branch miss rate: {perf['branch_miss_rate']}%\")
          "
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.client }}-${{ matrix.workload }}
          path: |
            benchmark_results_${{ matrix.client }}_${{ matrix.workload }}.json
            benchmark_results_${{ matrix.client }}_${{ matrix.workload }}.csv

  summary:
    needs: benchmark
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: results
          pattern: benchmark-results-*
          merge-multiple: true

      - name: Generate summary report
        run: |
          echo "# Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Client | Workload | Status | Elapsed |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|----------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          for f in results/*.json 2>/dev/null; do
            if [ -f "$f" ]; then
              python3 -c "
          import json
          with open('$f') as file:
              data = json.load(file)
          client = data['config']['client']['client_name']
          workload = data['config']['workload']['benchmark-profile']['name']
          elapsed = data['results']['elapsed_seconds']
          print(f'| {client} | {workload} | ✅ | {elapsed}s |')
          " >> $GITHUB_STEP_SUMMARY
            fi
          done || echo "| - | - | ❌ No results | - |" >> $GITHUB_STEP_SUMMARY