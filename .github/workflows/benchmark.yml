name: Benchmark

on:
  workflow_dispatch:
    inputs:
      driver:
        description: 'Driver config JSON path relative to resp-bench/configs/drivers/ (e.g., "example-jedis-standalone")'
        required: true
        type: string
        default: 'example-spring-data-valkey-glide-standalone'
      workload:
        description: 'Workload config JSON path relative to resp-bench/configs/workloads/ (e.g., "example-workload")'
        required: true
        type: string
        default: 'example-workload'
      primary_version:
        description: 'Primary driver version or commit ID. Must be EMPTY for spring-data-valkey (auto-resolved to branch HEAD).'
        required: false
        type: string
        default: ''
      secondary_version:
        description: 'Secondary driver version or commit ID. Must be EMPTY if driver config has no secondary driver.'
        required: false
        type: string
        default: ''
      job_id_prefix:
        description: 'Optional prefix for the job ID (e.g., "regression", "nightly", "pr-123")'
        required: false
        type: string
        default: ''

  push:
    branches:
      - add-benchmark-ci

env:
  DEFAULT_DRIVER: 'example-spring-data-valkey-glide-standalone'
  DEFAULT_WORKLOAD: 'example-workload'
  DEFAULT_PRIMARY_VERSION: ''
  DEFAULT_SECONDARY_VERSION: ''
  DEFAULT_JOB_ID_PREFIX: ''
  RESP_BENCH_REPO: 'https://github.com/ikolomi/resp-bench.git'

jobs:
  benchmark:
    runs-on: [self-hosted, Linux, x86, ephemeral, metal]
    timeout-minutes: 120

    steps:
      - uses: actions/checkout@v4

      - name: Set up Java 21 and Maven
        run: |
          sudo apt-get update
          sudo apt-get install -y openjdk-21-jdk-headless maven
          java -version
          mvn -version

      - name: Clone resp-bench
        run: |
          echo "Cloning resp-bench repository..."
          git clone --depth 1 ${{ env.RESP_BENCH_REPO }} resp-bench
          echo "✓ resp-bench cloned to $(pwd)/resp-bench"
          ls -la resp-bench/

      - name: Build resp-bench Java benchmark
        run: |
          cd resp-bench
          make java-build
          echo "✓ Java benchmark built"
          ls -la java/target/*.jar

      - name: Resolve inputs
        id: inputs
        run: |
          if [ "${{ github.event_name }}" = "push" ]; then
            DRIVER="${{ env.DEFAULT_DRIVER }}"
            WORKLOAD="${{ env.DEFAULT_WORKLOAD }}"
            PRIMARY_VERSION="${{ env.DEFAULT_PRIMARY_VERSION }}"
            SECONDARY_VERSION="${{ env.DEFAULT_SECONDARY_VERSION }}"
            JOB_ID_PREFIX="${{ env.DEFAULT_JOB_ID_PREFIX }}"
          else
            DRIVER="${{ github.event.inputs.driver }}"
            WORKLOAD="${{ github.event.inputs.workload }}"
            PRIMARY_VERSION="${{ github.event.inputs.primary_version }}"
            SECONDARY_VERSION="${{ github.event.inputs.secondary_version }}"
            JOB_ID_PREFIX="${{ github.event.inputs.job_id_prefix }}"
          fi

          echo "driver=$DRIVER" >> $GITHUB_OUTPUT
          echo "workload=$WORKLOAD" >> $GITHUB_OUTPUT
          echo "primary_version=$PRIMARY_VERSION" >> $GITHUB_OUTPUT
          echo "secondary_version=$SECONDARY_VERSION" >> $GITHUB_OUTPUT
          echo "job_id_prefix=$JOB_ID_PREFIX" >> $GITHUB_OUTPUT

          echo "========================================"
          echo "Driver:            $DRIVER"
          echo "Workload:          $WORKLOAD"
          echo "Primary version:   '${PRIMARY_VERSION}'"
          echo "Secondary version: '${SECONDARY_VERSION}'"
          echo "Job ID prefix:     '${JOB_ID_PREFIX}'"
          echo "========================================"

      - name: Validate configs exist
        run: |
          DRIVER="${{ steps.inputs.outputs.driver }}"
          WORKLOAD="${{ steps.inputs.outputs.workload }}"

          DRIVER_CONFIG="resp-bench/configs/drivers/${DRIVER}.json"
          WORKLOAD_CONFIG="resp-bench/configs/workloads/${WORKLOAD}.json"

          if [ ! -f "$DRIVER_CONFIG" ]; then
            echo "ERROR: Driver config not found: $DRIVER_CONFIG"
            echo "Available driver configs:"
            ls -la resp-bench/configs/drivers/
            exit 1
          fi
          echo "✓ Driver config: $DRIVER_CONFIG"

          if [ ! -f "$WORKLOAD_CONFIG" ]; then
            echo "ERROR: Workload config not found: $WORKLOAD_CONFIG"
            echo "Available workload configs:"
            ls -la resp-bench/configs/workloads/
            exit 1
          fi
          echo "✓ Workload config: $WORKLOAD_CONFIG"

          echo "Driver config contents:"
          cat "$DRIVER_CONFIG" | jq .

          echo "Workload phases:"
          jq '.phases[] | {id, completion}' "$WORKLOAD_CONFIG"

      - name: Validate and resolve versions
        id: versions
        run: |
          DRIVER="${{ steps.inputs.outputs.driver }}"
          PRIMARY_VERSION="${{ steps.inputs.outputs.primary_version }}"
          SECONDARY_VERSION="${{ steps.inputs.outputs.secondary_version }}"

          DRIVER_CONFIG="resp-bench/configs/drivers/${DRIVER}.json"
          DRIVER_ID=$(jq -r '.driver_id' "$DRIVER_CONFIG")
          SECONDARY_DRIVER_ID=$(jq -r '.specific_driver_config.secondary_driver_id // empty' "$DRIVER_CONFIG")

          echo "Driver ID:           $DRIVER_ID"
          echo "Secondary Driver ID: ${SECONDARY_DRIVER_ID:-<none>}"

          ERRORS=0

          # Rule 1: spring-data-valkey must NOT have explicit primary version
          if [ "$DRIVER_ID" = "spring-data-valkey" ] && [ -n "$PRIMARY_VERSION" ]; then
            echo "ERROR: primary_version must NOT be provided when driver_id is 'spring-data-valkey'."
            echo "       The branch HEAD commit SHA is used automatically."
            ERRORS=$((ERRORS + 1))
          fi

          # Rule 2: no secondary driver but secondary version provided
          if [ -z "$SECONDARY_DRIVER_ID" ] && [ -n "$SECONDARY_VERSION" ]; then
            echo "ERROR: secondary_version was provided but driver '$DRIVER' has no secondary driver."
            ERRORS=$((ERRORS + 1))
          fi

          if [ "$ERRORS" -gt 0 ]; then
            echo "========================================="
            echo "VALIDATION FAILED: $ERRORS error(s) found"
            echo "========================================="
            exit 1
          fi

          # Resolve spring-data-valkey: use branch HEAD commit SHA
          if [ "$DRIVER_ID" = "spring-data-valkey" ]; then
            PRIMARY_VERSION="${{ github.sha }}"
            echo "Resolved spring-data-valkey primary_version to HEAD: $PRIMARY_VERSION"
          fi

          # If no secondary driver exists, force secondary version to empty
          if [ -z "$SECONDARY_DRIVER_ID" ]; then
            SECONDARY_VERSION=""
          fi

          # Build versions JSON
          VERSIONS_JSON=$(jq -c -n \
            --arg pid "$DRIVER_ID" \
            --arg pv "$PRIMARY_VERSION" \
            --arg sid "$SECONDARY_DRIVER_ID" \
            --arg sv "$SECONDARY_VERSION" \
            '{
              primary_driver_id: $pid,
              primary_driver_version: (if $pv == "" then null else $pv end),
              secondary_driver_id: (if $sid == "" then null else $sid end),
              secondary_driver_version: (if $sv == "" then null else $sv end)
            }')

          echo ""
          echo "Resolved versions JSON:"
          echo "$VERSIONS_JSON" | jq .
          echo "versions_json=$VERSIONS_JSON" >> $GITHUB_OUTPUT
          echo "✓ All version validations passed"

      - name: Verify metal instance
        run: |
          echo "=== System Info ==="
          lscpu | grep "Model name"
          echo "CPU cores: $(nproc)"

          VIRT=$(systemd-detect-virt 2>/dev/null || echo "none")
          echo "Virtualization: $VIRT"
          if [ "$VIRT" != "none" ]; then
            echo "WARNING: Not running on bare metal!"
          fi

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y sysstat iproute2 python3-pip

          # Install Python dependencies system-wide
          sudo python3 -m pip install psycopg2-binary boto3 hdrhistogram

          # Install FlameGraph tools
          if [ ! -d "/opt/FlameGraph" ]; then
            git clone --depth 1 https://github.com/brendangregg/FlameGraph.git /tmp/FlameGraph
            sudo mv /tmp/FlameGraph /opt/FlameGraph
          fi
          ls /opt/FlameGraph/stackcollapse-perf.pl && echo "✓ FlameGraph tools installed"

          CODENAME=$(lsb_release -cs)
          KERNEL=$(uname -r)
          echo "Ubuntu: $CODENAME, Kernel: $KERNEL"

          if sudo apt-get install -y linux-tools-$(uname -r) 2>/dev/null; then
            echo "✓ Installed kernel-matched perf tools"
          else
            echo "Kernel-matched tools not available, trying HWE fallback..."

            cd /tmp

            KERNEL_MAJOR=$(echo $KERNEL | cut -d'.' -f1,2)
            echo "Looking for HWE tools for kernel $KERNEL_MAJOR..."

            HWE_PKG=$(apt-cache search "linux-hwe.*tools.*${KERNEL_MAJOR}" 2>/dev/null | \
                      grep -oP "linux-hwe-[\d.]+-tools-[\d.]+-\d+" | \
                      sort -V | tail -1)

            if [ -n "$HWE_PKG" ]; then
              echo "Found HWE package: $HWE_PKG"
              apt-get download "$HWE_PKG"
            else
              echo "No matching HWE package found, trying generic..."
              sudo apt-get install -y linux-tools-generic 2>/dev/null || true
            fi

            for deb in linux-hwe*tools*.deb; do
              if [ -f "$deb" ]; then
                echo "Extracting $deb..."
                sudo mkdir -p /opt/perf-tools
                sudo dpkg -x "$deb" /opt/perf-tools
                PERF_BIN=$(find /opt/perf-tools -name "perf" -type f 2>/dev/null | head -1)
                if [ -n "$PERF_BIN" ]; then
                  sudo ln -sf "$PERF_BIN" /usr/local/bin/perf
                  echo "✓ Linked perf from $PERF_BIN"
                fi
                rm -f "$deb"
                break
              fi
            done

            cd -
          fi

          # Set perf permissions before verification
          sudo sysctl -w kernel.perf_event_paranoid=-1
          sudo sysctl -w kernel.kptr_restrict=0

          echo ""
          echo "=== Perf Verification ==="
          which perf
          perf --version
          perf stat -e task-clock -- sleep 0.1
          echo "✓ Perf installed and working"

      - name: Run benchmark
        run: |
          DRIVER="${{ steps.inputs.outputs.driver }}"
          WORKLOAD="${{ steps.inputs.outputs.workload }}"
          JOB_ID_PREFIX="${{ steps.inputs.outputs.job_id_prefix }}"
          VERSIONS_JSON='${{ steps.versions.outputs.versions_json }}'

          if [ -n "$JOB_ID_PREFIX" ]; then
            python3 .github/scripts/benchmark_runner.py \
              --output "benchmark_results_${DRIVER}_${WORKLOAD}.json" \
              --workload-config "resp-bench/configs/workloads/${WORKLOAD}.json" \
              --driver-config "resp-bench/configs/drivers/${DRIVER}.json" \
              --resp-bench-dir "${{ github.workspace }}/resp-bench" \
              --versions-json "$VERSIONS_JSON" \
              --job-id-prefix "$JOB_ID_PREFIX"
          else
            python3 .github/scripts/benchmark_runner.py \
              --output "benchmark_results_${DRIVER}_${WORKLOAD}.json" \
              --workload-config "resp-bench/configs/workloads/${WORKLOAD}.json" \
              --driver-config "resp-bench/configs/drivers/${DRIVER}.json" \
              --resp-bench-dir "${{ github.workspace }}/resp-bench" \
              --versions-json "$VERSIONS_JSON"
          fi

      - name: Display results summary
        if: always()
        run: |
          DRIVER="${{ steps.inputs.outputs.driver }}"
          WORKLOAD="${{ steps.inputs.outputs.workload }}"
          RESULT_FILE="benchmark_results_${DRIVER}_${WORKLOAD}.json"

          if [ -f "$RESULT_FILE" ]; then
            echo "=== Benchmark Results Summary ==="
            python3 -c "
          import json
          with open('$RESULT_FILE') as f:
              data = json.load(f)

          print(f\"Job ID: {data['job_id']}\")
          print(f\"Driver: {data['config']['driver']['driver_id']}\")
          print(f\"Workload: {data['config']['workload']['benchmark_profile']['name']}\")
          print(f\"Elapsed: {data['results']['elapsed_ms']}ms\")
          print()

          versions = data.get('versions', {})
          print(f\"Primary:   {versions.get('primary_driver_id')} @ {versions.get('primary_driver_version', 'N/A')}\")
          print(f\"Secondary: {versions.get('secondary_driver_id', 'N/A')} @ {versions.get('secondary_driver_version', 'N/A')}\")
          print()

          steady = data['results']['phases'].get('STEADY', {})
          if steady:
              totals = steady.get('totals', {})
              print(f\"STEADY totals: {totals.get('requests', 0):,} requests, {totals.get('errors', 0)} errors\")
              print()
              print('Per-command metrics:')
              for cmd, m in steady.get('metrics', {}).items():
                  reqs = m['requests']
                  errs = m['errors']
                  lat = m.get('latency', {})
                  summary = lat.get('summary', {})
                  print(f\"  {cmd}:\")
                  print(f\"    Requests: {reqs:,} (errors: {errs})\")
                  print(f\"    Latency: min={summary.get('min')} p50={summary.get('p50')} p95={summary.get('p95')} p99={summary.get('p99')} p999={summary.get('p999')} max={summary.get('max')} us\")

          print()
          print('Perf:')
          perf = data['results']['perf']['counters']
          if perf.get('ipc'):
              print(f\"  IPC: {perf['ipc']}\")
          if perf.get('cache_miss_rate'):
              print(f\"  Cache miss rate: {perf['cache_miss_rate']}%\")
          if perf.get('branch_miss_rate'):
              print(f\"  Branch miss rate: {perf['branch_miss_rate']}%\")
          "
          else
            echo "⚠ Result file not found: $RESULT_FILE"
          fi

      - name: Generate summary report
        if: always()
        run: |
          DRIVER="${{ steps.inputs.outputs.driver }}"
          WORKLOAD="${{ steps.inputs.outputs.workload }}"
          RESULT_FILE="benchmark_results_${DRIVER}_${WORKLOAD}.json"

          echo "# Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "$RESULT_FILE" ]; then
            python3 << EOF >> $GITHUB_STEP_SUMMARY
          import json
          with open("$RESULT_FILE") as f:
              data = json.load(f)

          versions = data.get("versions", {})
          driver = data["config"]["driver"]["driver_id"]
          workload_profile = data["config"]["workload"]["benchmark_profile"]["name"]
          elapsed = data["results"]["elapsed_ms"]
          pv = versions.get("primary_driver_version") or "N/A"
          sv = versions.get("secondary_driver_version")
          sid = versions.get("secondary_driver_id")

          print(f"**Driver:** {driver} @ \`{pv}\`")
          if sid and sv:
              print(f"**Secondary:** {sid} @ \`{sv}\`")
          print(f"**Workload config:** \`${WORKLOAD}\` ({workload_profile})")
          print(f"**Driver config:** \`${DRIVER}\`")
          print(f"**Elapsed:** {elapsed}ms")
          print()

          print("| Versions | Driver | Workload | Status | Elapsed |")
          print("|----------|--------|----------|--------|---------|")
          version_str = pv
          if sv:
              version_str += f" ({sid}: {sv})"
          print(f"| {version_str} | {driver} | \`${WORKLOAD}\` | ✅ | {elapsed}ms |")
          print()

          steady = data["results"]["phases"].get("STEADY", {})
          if steady:
              print("### Steady State Metrics")
              print("| Command | Requests | Errors | p50 (μs) | p95 (μs) | p99 (μs) | p999 (μs) |")
              print("|---------|----------|--------|----------|----------|----------|-----------|")
              for cmd, m in steady.get("metrics", {}).items():
                  s = m.get("latency", {}).get("summary", {})
                  print(f"| {cmd} | {m['requests']:,} | {m['errors']} | {s.get('p50','-')} | {s.get('p95','-')} | {s.get('p99','-')} | {s.get('p999','-')} |")
          EOF
          else
            echo "❌ No results found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ steps.inputs.outputs.driver }}-${{ steps.inputs.outputs.workload }}
          path: |
            benchmark_results_${{ steps.inputs.outputs.driver }}_${{ steps.inputs.outputs.workload }}.json
            benchmark_results_${{ steps.inputs.outputs.driver }}_${{ steps.inputs.outputs.workload }}.ndjson
